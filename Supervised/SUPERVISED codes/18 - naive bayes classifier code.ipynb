{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f93602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.prior = None\n",
    "        self.likelihood = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Calculate class priors\n",
    "        self.prior = {}\n",
    "        for label in np.unique(y):\n",
    "            self.prior[label] = np.sum(y == label) / len(y)\n",
    "        \n",
    "        # Calculate likelihoods\n",
    "        self.likelihood = {}\n",
    "        for label in np.unique(y):\n",
    "            self.likelihood[label] = {}\n",
    "            for feature in range(X.shape[1]):\n",
    "                # Calculate P(feature|class)\n",
    "                self.likelihood[label][feature] = {}\n",
    "                for value in np.unique(X[:, feature]):\n",
    "                    self.likelihood[label][feature][value] = \\\n",
    "                        np.sum((X[:, feature] == value) & (y == label)) / np.sum(y == label)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for i in range(X.shape[0]):\n",
    "            # Calculate posterior for each class\n",
    "            posteriors = {}\n",
    "            for label in self.prior:\n",
    "                posterior = np.log(self.prior[label])\n",
    "                for feature in range(X.shape[1]):\n",
    "                    posterior += np.log(self.likelihood[label][feature].get(X[i, feature], 0.01))  # Smoothing\n",
    "                posteriors[label] = posterior\n",
    "            # Predict the class with the highest posterior\n",
    "            predictions.append(max(posteriors, key=posteriors.get))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3f19b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp5cd\\AppData\\Local\\Temp\\ipykernel_28136\\308358968.py:33: RuntimeWarning: divide by zero encountered in log\n",
      "  posterior += np.log(self.likelihood[label][feature].get(X[i, feature], 0.01))  # Smoothing\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "X_train = np.array([[1, 0], [1, 1], [0, 1], [0, 0], [1, 1]])\n",
    "y_train = np.array([0, 1, 1, 0, 1])\n",
    "\n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "X_test = np.array([[1, 0], [0, 1], [1, 1]])\n",
    "predictions = nb_classifier.predict(X_test)\n",
    "print(predictions)  # Output: [0, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9f1ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = (y_pred == y_test).sum() / len(y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1852fc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Sample text data\n",
    "X = [\"I love to eat pizza\", \"Pizza is the best\", \"I hate broccoli\"]\n",
    "y = [1, 1, 0]  # 1 for positive sentiment, 0 for negative sentiment\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = CountVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_vectorized, y)\n",
    "\n",
    "# Test\n",
    "new_text = [\"Broccoli is disgusting\"]\n",
    "new_text_vectorized = vectorizer.transform(new_text)\n",
    "prediction = mnb.predict(new_text_vectorized)\n",
    "print(\"Prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc33c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Complement Naive Bayes\n",
    "cnb = ComplementNB()\n",
    "cnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = cnb.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = (y_pred == y_test).sum() / len(y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cbc9bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Sample binary data\n",
    "X = [[1, 0], [0, 1], [1, 1], [0, 0]]\n",
    "y = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Bernoulli Naive Bayes\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X, y)\n",
    "\n",
    "# Test\n",
    "new_data = [[1, 0], [1, 1]]\n",
    "predictions = bnb.predict(new_data)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5bfb364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "# Sample data\n",
    "X = [[0, 1, 2], [1, 2, 2], [2, 1, 0], [0, 1, 1]]\n",
    "y = [0, 1, 1, 0]  # Classes\n",
    "\n",
    "# Categorical Naive Bayes\n",
    "cat_nb = CategoricalNB()\n",
    "cat_nb.fit(X, y)\n",
    "\n",
    "# Test\n",
    "new_data = [[0, 1, 1], [2, 1, 0]]\n",
    "predictions = cat_nb.predict(new_data)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6ab3c",
   "metadata": {},
   "source": [
    "Certainly! Here are some common applications of each Naive Bayes algorithm:\n",
    "\n",
    "### 1. Gaussian Naive Bayes\n",
    "- **Text Classification**: Gaussian Naive Bayes can be used for text classification tasks where features are continuous variables, such as sentiment analysis or spam detection. For example, it can classify documents based on the frequency of certain words.\n",
    "- **Medical Diagnosis**: It can be used in medical diagnosis tasks where features are continuous, such as the levels of different biomarkers, to classify patients into different disease categories.\n",
    "\n",
    "### 1.9.2. Multinomial Naive Bayes\n",
    "- **Text Classification**: Multinomial Naive Bayes is particularly suitable for text classification tasks where features represent word counts or frequencies. It's commonly used in spam filtering, topic classification, and sentiment analysis.\n",
    "- **Document Classification**: It's often used in document categorization tasks, such as news article classification or document clustering.\n",
    "\n",
    "### 1.9.3. Complement Naive Bayes\n",
    "- **Text Classification**: Complement Naive Bayes is particularly effective for text classification tasks with imbalanced class distributions. It's often used in sentiment analysis, topic classification, and document categorization tasks.\n",
    "- **Spam Filtering**: It can be used in spam filtering where the spam class is significantly smaller than the non-spam class.\n",
    "\n",
    "### 1.9.4. Bernoulli Naive Bayes\n",
    "- **Text Classification**: Bernoulli Naive Bayes is commonly used for binary feature classification tasks, such as document classification where features represent the presence or absence of words in the document.\n",
    "- **Sentiment Analysis**: It's used in sentiment analysis tasks where features represent the occurrence of specific words or features in the text.\n",
    "\n",
    "### 1.9.5. Categorical Naive Bayes\n",
    "- **Data Classification**: Categorical Naive Bayes is useful for classification tasks where features are categorical variables with discrete values. It can be used in various domains such as marketing (e.g., customer segmentation), finance (e.g., credit risk assessment), and healthcare (e.g., disease diagnosis).\n",
    "\n",
    "### 1.9.6. Out-of-core Naive Bayes\n",
    "- **Large-scale Text Classification**: Out-of-core Naive Bayes is suitable for large-scale text classification tasks where the dataset cannot fit into memory. It's commonly used in scenarios like processing large volumes of social media data, news articles, or emails for classification tasks.\n",
    "- **Real-time Classification**: It can be used in real-time classification scenarios where new data arrives continuously, and the model needs to be updated incrementally without retraining on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c50f76e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy Rate: 0.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.00      0.00      0.00       1.0\n",
      "        Spam       0.00      0.00      0.00       1.0\n",
      "\n",
      "    accuracy                           0.00       2.0\n",
      "   macro avg       0.00      0.00      0.00       2.0\n",
      "weighted avg       0.00      0.00      0.00       2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define email data and tags\n",
    "emails = [\n",
    "    \"Will you be attending today's meeting?\",\n",
    "    \"Don't miss out, it's full of great discounts!\",\n",
    "    \"Don't forget to install the update\",\n",
    "    \"Last day: 50% off!\",\n",
    "    \"Last call, renew your subscription\",\n",
    "    \"Where to meet friends?\"\n",
    "]\n",
    "\n",
    "labels = [0, 1, 0, 1, 1, 0]  # 0: Normal, 1: Spam\n",
    "\n",
    "# Convert text to vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(emails)\n",
    "\n",
    "# Separate data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and train a Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict using test data\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the success of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy Rate:\", accuracy)\n",
    "\n",
    "# Show classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Normal\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30e27c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
